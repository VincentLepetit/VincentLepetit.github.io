{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_introduction2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A4KiiK1--35f"
      },
      "source": [
        "## Deep Learning: Introduction (2)\n",
        "\n",
        "The goal of this second exercice is to see a Convolutional Network for classification in PyTorch (and to discover the joy of tensor dimensioning)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f52i2H1AUV1p"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# PyTorch:\n",
        "import torch\n",
        "\n",
        "# For visualization:\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcItiQZt_dpU"
      },
      "source": [
        "The functions below will be used to create a simple dataset of synthetic images. It is not important to understand them. What matters is to notice the size of the generated images: 72x72"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WI-4ll27UwJU"
      },
      "source": [
        "def generate_a_drawing(U, V, noise=0.0):\n",
        "    figsize = 1.0    \n",
        "    fig = plt.figure(figsize=(figsize,figsize))\n",
        "    ax = plt.subplot(111)\n",
        "    plt.axis('Off')\n",
        "    ax.set_xlim(0,figsize)\n",
        "    ax.set_ylim(0,figsize)\n",
        "    ax.fill(U, V, \"k\")\n",
        "    fig.canvas.draw()\n",
        "    imdata = np.frombuffer(fig.canvas.tostring_rgb(), dtype=np.uint8)[::3].astype(np.float32)\n",
        "    imdata = imdata + noise * np.random.random(imdata.size)\n",
        "    plt.close(fig)\n",
        "    return imdata.reshape(72,72)\n",
        "\n",
        "def generate_a_rectangle(noise=0.0):\n",
        "    U = np.zeros(4)\n",
        "    V = np.zeros(4)\n",
        "    corners = np.random.random(4)\n",
        "    top = max(corners[0], corners[1])\n",
        "    bottom = min(corners[0], corners[1])\n",
        "    left = min(corners[2], corners[3])\n",
        "    right = max(corners[2], corners[3])\n",
        "    U[0] = U[1] = top\n",
        "    U[2] = U[3] = bottom\n",
        "    V[0] = V[3] = left\n",
        "    V[1] = V[2] = right\n",
        "    return generate_a_drawing(U, V, noise)\n",
        "\n",
        "def generate_a_disk(noise=0.0):\n",
        "    center = np.random.random(2)\n",
        "    radius = (0.3 + 0.7 * np.random.random()) / 2\n",
        "    N = 50\n",
        "    U = np.zeros(N)\n",
        "    V = np.zeros(N)\n",
        "    i = 0\n",
        "    for t in np.linspace(0, 2*np.pi, N):\n",
        "        U[i] = center[0] + np.cos(t) * radius\n",
        "        V[i] = center[1] + np.sin(t) * radius\n",
        "        i = i + 1\n",
        "    return generate_a_drawing(U, V, noise)\n",
        "\n",
        "def generate_a_triangle(noise=0.0):\n",
        "    figsize = 1.0\n",
        "    U = np.random.random(3)\n",
        "    V = np.random.random(3)\n",
        "    return generate_a_drawing(U, V, noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8iiY7cdt_oAI"
      },
      "source": [
        "Checking the functions above:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTLPOnzjU3Ig"
      },
      "source": [
        "im = generate_a_rectangle(10)\n",
        "plt.imshow(im, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pIP97t_U6rW"
      },
      "source": [
        "im = generate_a_disk(10)\n",
        "plt.imshow(im, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZvfKJGX9VMwQ"
      },
      "source": [
        "im = generate_a_triangle(50)\n",
        "plt.imshow(im, cmap='gray')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aG18qRzf_wGd"
      },
      "source": [
        "In PyTorch, we can create a Dataset class to store and access the samples of a dataset:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w8DNeZGAXfss"
      },
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, utils\n",
        "\n",
        "class ShapesDataset(Dataset):\n",
        "    def __init__(self, nb_samples, noise=0.0, transform=None):\n",
        "        # Getting im_size:\n",
        "        im_size = generate_a_rectangle().shape[0]\n",
        "        self.X = np.zeros([nb_samples,72,72])\n",
        "        self.Y = np.zeros(nb_samples, dtype=np.int)\n",
        "        print('Creating data:')\n",
        "        for i in range(nb_samples):\n",
        "            if i % 10 == 0:\n",
        "                print(i)\n",
        "            category = np.random.randint(3)\n",
        "            if category == 0:\n",
        "                self.X[i] = generate_a_rectangle(noise)\n",
        "            elif category == 1: \n",
        "                self.X[i] = generate_a_disk(noise)\n",
        "            else:\n",
        "                self.X[i] = generate_a_triangle(noise)\n",
        "            self.Y[i] = category\n",
        "        # Normalizing the intensities to be between 0 and 1:\n",
        "        self.X = (self.X + noise) / (255 + 2 * noise)\n",
        "        # Transformation to apply to the samples \n",
        "        # (see below, we will use it to  transform the images to PyTorch tensors):\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # idx can be a set of indices stored into a PyTorch tensor:\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        im = self.X[idx]\n",
        "        if self.transform:\n",
        "          im = self.transform(im)\n",
        "        return im, self.Y[idx]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGwZJM2qAn3C"
      },
      "source": [
        "Let's test the ShapesDataset class:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vB3nehdwkAqf"
      },
      "source": [
        "set = ShapesDataset(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NdB31YkAyCW"
      },
      "source": [
        "im, category = set[0]\n",
        "plt.imshow(im, cmap='gray')\n",
        "print(category)\n",
        "print(im.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olb16eFwAxij"
      },
      "source": [
        "We now create a class for our Convolutional Network. It has a single convolutional layer, a pooling layer, and a fully connected layer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pdPanoLznJ9g"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as func\n",
        "\n",
        "class ConvNet(torch.nn.Module): \n",
        "    def __init__(self):\n",
        "        super(ConvNet, self).__init__()\n",
        "        # 6 5Ã—5 filters applied to the input image: \n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        # (72-4) / 2 = 34\n",
        "        self.fc1 = nn.Linear(34*34*6, 3)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = func.relu(self.conv1(x))\n",
        "        x = func.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 34*34*6)\n",
        "        x = self.fc1(x)\n",
        "        return func.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zmSn8jyGBQqk"
      },
      "source": [
        "- Make sure you understand the code.\n",
        "- Understand where the values `34*34*6` and `3` come from in the last row of the `__init__` function.\n",
        "- What does the `view` function do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAiWoRl2B1Ho"
      },
      "source": [
        "We can use our `ShapesDataset` class to create a training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ic_1u2BwB79U"
      },
      "source": [
        "training_set = ShapesDataset(100, transform=transforms.ToTensor())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rH0i0C7JCAo5"
      },
      "source": [
        "We can instanciate a 'data loader', which will be used in the optimization to sample batches from the training set:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqkmKdp8pRmH"
      },
      "source": [
        "trainloader = torch.utils.data.DataLoader(training_set, batch_size=3, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdDZLzvQCT_x"
      },
      "source": [
        "We can use the function to train out network over 1 epoch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTdIQOJrpphP"
      },
      "source": [
        "def train_one_epoch(model, trainloader, optimizer, epoch): \n",
        "    for batch_id, (images, labels) in enumerate(trainloader):\n",
        "        optimizer.zero_grad()\n",
        "        # model(..) calls the forward function, which expects float values:\n",
        "        predictions = model(images.float())\n",
        "        # nll stands for negative log likelihood:\n",
        "        loss = func.nll_loss(predictions, labels)\n",
        "        # the backward function computes the network parameters' gradients \n",
        "        # that will be used by the optimizer. It is inherited from `torch.nn.Module`.\n",
        "        loss.backward()\n",
        "        # 1 optimization step: \n",
        "        optimizer.step()\n",
        "        if batch_id % 100 == 0:\n",
        "            print('loss: ', loss.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9JxVezdDA6_"
      },
      "source": [
        "Let's instantiate a `ConvNet` and an Adam optimizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAWC-PfRptYI"
      },
      "source": [
        "model = ConvNet()\n",
        "optimizer = torch.optim.Adam(model.parameters())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ey8Bnk8jDIoh"
      },
      "source": [
        "Let's run a few optimization steps:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bQte-nUDOqv",
        "outputId": "534245e1-94ca-413c-efac-e0696f27daa1"
      },
      "source": [
        "for epoch in range(0, 5):\n",
        "    train_one_epoch(model, trainloader, optimizer, epoch)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loss:  0.40867510437965393\n",
            "loss:  0.24257434904575348\n",
            "loss:  0.0975199043750763\n",
            "loss:  0.098531074821949\n",
            "loss:  0.06292534619569778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWJoO_2CDeGt"
      },
      "source": [
        "We can apply our model to new data as detailed below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1EL-MpMDmUp"
      },
      "source": [
        "# Creating a new sample:\n",
        "set = ShapesDataset(1)\n",
        "im, category = set[0]\n",
        "plt.imshow(im, cmap='gray')\n",
        "print(category)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2bjDj6yDvaN"
      },
      "source": [
        "Calling `model(im)` will apply our network to image `im` we just created by callling the `forward()` function in our `ConvNet` class. BUT the `conv2d` function at the beginning of `forward()` expects a tensor of 4 dimensions:\n",
        "\n",
        "`nbsamples x channels x height x width`\n",
        "\n",
        "while `im` is currently a numpy array of dimensions:\n",
        "\n",
        "`height x width`\n",
        "\n",
        "We can first transform `im` into a PyTorch tensor:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5gdmarUFJyf"
      },
      "source": [
        "im = torch.Tensor(im)\n",
        "print(im)\n",
        "print(im.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mu5wOs66FS44"
      },
      "source": [
        "We can transform it into a 3-tensor  of dimensions\n",
        "\n",
        "`1 x height x width`\n",
        "\n",
        "using the `unsqueeze` function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cnv_73Px4GtU"
      },
      "source": [
        "# 0 means we want to add the dimension at the beginning:\n",
        "im = im.unsqueeze(0)\n",
        "print(im)\n",
        "print(im.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWwnDOH2GfGR"
      },
      "source": [
        "We need one more dimension at the beginning of `im`, so we call `unsqueeze` one more time:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KufuY2-8Gsk8"
      },
      "source": [
        "im = im.unsqueeze(0)\n",
        "print(im)\n",
        "print(im.size())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYdDownLG4iH"
      },
      "source": [
        "`im` has now the correct dimensions to apply our network to it:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqKhq0bnxhRF"
      },
      "source": [
        "model(im)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFcUwdAjHKMi"
      },
      "source": [
        "Can you interpret the output?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rph8zIFxIG-_"
      },
      "source": [
        "## Changing the architecture\n",
        "\n",
        "- Add a second convolutional layer and a second pooling layer. Decide what should be the size and number of the filters, and the size of the pooling regions. Note this has an impact of the dimensions for the fully connected layer. Test your code by optimizing the new network.\n",
        "- Add a second fully connected layer. Make sure the dimensions of the different layers are consistent together!  Test your code by optimizing the new network."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D4vRNPuC9qCT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}