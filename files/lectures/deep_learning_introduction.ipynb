{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "deep_learning_introduction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOHyV8JA1C7q"
      },
      "source": [
        "## Deep Learning: Introduction\n",
        "\n",
        "The goal of this exercice is to discover PyTorch. We will start with a simple 2D example for regression. You will have the opportunity to play with the network architecture and the optimization algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SteoKgYIiUIC"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# PyTorch:\n",
        "import torch\n",
        "\n",
        "# For visualization:\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2KEl0aTO1xbJ"
      },
      "source": [
        "### A 2D function\n",
        "We will first approximate the following 2D function with a 2-layer network:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nlmGPjsz2FcM"
      },
      "source": [
        "def F(x1, x2):\n",
        "  return np.sin(x1) * np.sin(x2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhH4YWPa2MYf"
      },
      "source": [
        "You can use the code below to visualize the function:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohf0N0W52Rue"
      },
      "source": [
        "def show_fn(fn, A):\n",
        "  Size = 200\n",
        "  step = 2 * A / Size\n",
        "  U,V = np.mgrid[-A:+A:step, -A:+A:step]\n",
        "  fn_X = fn(U, V)\n",
        "  I = fn_X.reshape(Size, Size)\n",
        "  plt.imshow(I, cmap = cm.Greys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z9wJm_Jj2VK6"
      },
      "source": [
        "show_fn(F, np.pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YMWQzUFQ2kZW"
      },
      "source": [
        "We will now generate training samples using this function. Make sure you understand the code."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1b8s_PO1iYx5"
      },
      "source": [
        "nb_samples = 1000\n",
        "X_train = 2 * A * torch.rand(nb_samples, 2) - A \n",
        "y_train = np.vectorize(F)(X_train[:,0], X_train[:,1])\n",
        "y_train = torch.tensor(y_train)\n",
        "print(X_train[0:10])\n",
        "print(y_train[0:10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXMEnXpf3MHJ"
      },
      "source": [
        "### First prototype\n",
        "\n",
        "Let's try a first prototype of a 2-layer network. This network has the structure:\n",
        "\n",
        "$h_1 = \\text{ReLU}(W_1 x) \\> , h_2 = w_2^\\top h_1$\n",
        "\n",
        "(note there is no bias yet)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FcgCMyQVlAk_"
      },
      "source": [
        "nb_neurons = 3\n",
        "W1 = torch.randn(2, nb_neurons, requires_grad=True)\n",
        "W2 = torch.randn(nb_neurons, 1, requires_grad=True)\n",
        "\n",
        "# X_train.mm(W1) is xW1\n",
        "# (PyTorch uses right side multiplication)\n",
        "# here, W1 is applied to all the samples: \n",
        "h1 = X_train.mm(W1)\n",
        "# clamp(min=0) is ReLU:\n",
        "h1 = h1.clamp(min=0)\n",
        "\n",
        "h2 = h1.mm(W2)\n",
        "\n",
        "loss = (h2 - y_train).pow(2).sum() / nb_samples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL6dyN1r5CS5"
      },
      "source": [
        "**Question:** What are the size and type of $h_1$, $h_2$, and $loss$?  (Hint: you can use the .size() function)\n",
        "\n",
        "**Question:** What is the current value of the loss? (Hint: you have to use the .item() function)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vJtrydKF5Z15"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBysvM_65oYI"
      },
      "source": [
        "### First optimization\n",
        "\n",
        "Let's try to optimize our network with gradient descent:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pUMBUuN8mA4U"
      },
      "source": [
        "learning_rate = 1e-5\n",
        "for t in range(10):\n",
        "  # the same code as before but in 1 line only:\n",
        "  h1 = X_train.mm(W1).clamp(min=0)\n",
        "  h2 = h1.mm(W2)\n",
        "  loss = (h2 - y_train).pow(2).sum() / nb_samples\n",
        "  print(t, loss.item())\n",
        "  # Computes all the partial derivatives:\n",
        "  loss.backward()\n",
        "  # Does not include these computations in the derivative graph: \n",
        "  with torch.no_grad():\n",
        "    W1 -= learning_rate * W1.grad\n",
        "    W2 -= learning_rate * W2.grad\n",
        "    # .grad.zero_() zeroes the gradients after calling backward()\n",
        "    # Required because AutoGrad does not simply\n",
        "    # replace the gradient values but accumulates (sums) them: \n",
        "    W1.grad.zero_()\n",
        "    W2.grad.zero_()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ImOl57LC6F5U"
      },
      "source": [
        "**Make sure you understand the code above!**\n",
        "\n",
        "**Question:** What happens when you use 1e-6 as learning rate?  1e-2?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vdxcHQK76UIB"
      },
      "source": [
        "### Adding the biases\n",
        "\n",
        "**Question:** Starting from the code below, add the biases to the architecture of our network ie the network should now have the structure:\n",
        "\n",
        "$h_1 = \\text{ReLU}(W_1 x + b_1) \\> , h_2 = w_2^\\top h_1 + b_2$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tFrQD_GrobON"
      },
      "source": [
        "nb_neurons = 3\n",
        "W1 = torch.randn(2, nb_neurons, requires_grad=True)\n",
        "#b1 = ... What should be the size of b1?\n",
        "W2 = torch.randn(nb_neurons, 1, requires_grad=True)\n",
        "#b2 = ...\n",
        "ones = torch.ones(nb_samples,1)\n",
        "\n",
        "learning_rate = 1e-5\n",
        "for t in range(10):\n",
        "  # Note the use of ones. Why is it needed?\n",
        "  b1_ = ones.mm(b1)\n",
        "  # Change this line to introduce b1_:\n",
        "  h1 = X_train.mm(W1).clamp(min=0)\n",
        "  # Change this line to introduce b2_:\n",
        "  h2 = h1.mm(W2)\n",
        "  loss = (h2 - y_train).pow(2).sum() / nb_samples\n",
        "  print(t, loss.item())\n",
        "  # Computes all the partial derivatives:\n",
        "  loss.backward()\n",
        "  # Does not include these computations in the derivative graph: \n",
        "  with torch.no_grad():\n",
        "    W1 -= learning_rate * W1.grad\n",
        "    W2 -= learning_rate * W2.grad\n",
        "    # What do you have to add here?\n",
        "\n",
        "    # .grad.zero_() zeroes the gradients after calling backward()\n",
        "    # Required because AutoGrad does not simply\n",
        "    # replace the gradient values but accumulates (sums) them: \n",
        "    W1.grad.zero_()\n",
        "    W2.grad.zero_()\n",
        "    # What do you have to add here?\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDkzKP1B66CO"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6jVRMI9w9UKZ"
      },
      "source": [
        "### Using a class from PyTorch\n",
        "\n",
        "We can use classes from PyTorch for standard architectures:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UXLvrGtpAob"
      },
      "source": [
        "nb_samples = 1000\n",
        "X_train = 2 * np.pi * torch.rand(nb_samples, 2) - np.pi\n",
        "y_train = np.vectorize(F)(X_train[:,0], X_train[:,1])\n",
        "y_train = torch.tensor(y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fy4aamgpAZEQ"
      },
      "source": [
        "nb_neurons = 3\n",
        "twolayer_net = torch.nn.Sequential(\n",
        "  torch.nn.Linear(2, nb_neurons),\n",
        "  torch.nn.ReLU(),\n",
        "  torch.nn.Linear(nb_neurons, 1),\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bAqLxmBc988z"
      },
      "source": [
        "We can also use a predefined loss function (MSE stands for Mean Squared Error):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hgf_C_XSpS2_"
      },
      "source": [
        "loss_fn = torch.nn.MSELoss(reduction='sum')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l633hHMB-Vuj"
      },
      "source": [
        "and we can use an optimization method implemented by PyTorch:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "87bU153rpVDD"
      },
      "source": [
        "# Using stochastic gradient descent for optimization:\n",
        "optimizer = torch.optim.SGD(twolayer_net.parameters(), lr = 1e-7)\n",
        "for t in range(10):\n",
        "  # Forward pass:\n",
        "  y_pred = twolayer_net(X_train)\n",
        "  # Computes loss:\n",
        "  loss = loss_fn(y_pred, y_train) \n",
        "  print(t, loss.item())\n",
        "  # Computes all the partial derivatives:\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  # 1 iteration step of the optimizer:\n",
        "  optimizer.step()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUNXqndX_PPB"
      },
      "source": [
        "**Question:** Try to get a good approximation of the F function. You can use the code below to visualise the approximation by the network.\n",
        "\n",
        "Things you can do:\n",
        "- increase the number of iterations;\n",
        "- change the number of neurons;\n",
        "- change the optimizer (try using Adam);\n",
        "- add more layers.\n",
        "\n",
        "If the optimization diverges (ie you get nan for the loss function), you have to reinitialize the network by running the line `twolayer_net = ...` again.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1ZCLbzGptJE"
      },
      "source": [
        "def show_nn(nn, A):\n",
        "  Size = 200\n",
        "  step = 2.0 * A / Size\n",
        "  U,V = np.mgrid[-A:+A:step, -A:+A:step]\n",
        "  UV = np.vstack((U.flatten(), V.flatten())).T\n",
        "  nn_X = nn(torch.tensor(UV).float()).detach().numpy()\n",
        "  I = nn_X.reshape(Size, Size)\n",
        "  plt.imshow(I, cmap = cm.Greys)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bU5CCRf_xpSI"
      },
      "source": [
        "show_nn(twolayer_net, np.pi)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_cien58LAsnY"
      },
      "source": [
        "### Creating our own class\n",
        "\n",
        "We can encapsulate our code into a class inheriting from `torch.nn.Module`:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrD_ZGQcvT2u"
      },
      "source": [
        "class TwoLayerNet(torch.nn.Module): \n",
        "  def __init__(self, D_in, H, D_out):\n",
        "    super(TwoLayerNet, self).__init__()\n",
        "    self.linear1 = torch.nn.Linear(D_in, H)\n",
        "    self.linear2 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "  def forward(self, x):\n",
        "    h_relu = self.linear1(x).clamp(min=0) \n",
        "    y_pred = self.linear2(h_relu)\n",
        "    return y_pred\n",
        "\n",
        "    \n",
        "# Instantiates the class defined defined above:\n",
        "twolayer_net = TwoLayerNet(2, 20, 1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqti4ICHBHi5"
      },
      "source": [
        "The rest of the code remains the same."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e6HMy2ngDF7d"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}